---
title: "Week5_datingNLP"
output: html_notebook
date: '2022-05-25'
Naama Shenberger ID:211983747
Ella Hanzin ID:212028971
---
```{r}
install.packages("stringr")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("mosaic")
install.packages("dplyr")
install.packages("stringr")
install.packages("xtable")
install.packages("gridExtra")
install.packages("stopwords")
install.packages("quanteda")
```


#librarys
```{r}
library(stringr)
library(dplyr)
library(ggplot2)
library(mosaic)
library(dplyr)
library(stringr)
library(xtable)
library(gridExtra)
library(stopwords)
library(quanteda)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

The exercise deals with data from daiting web in the United States.

#Load texts
```{r}
profiles <- read.csv( file.path( 'profiles.csv' ), header=TRUE, stringsAsFactors=FALSE)
```


#Clean texts, tokenize,  (token, tokens_select)
```{r}
essays <- select(profiles, starts_with("essay"))
essays <- apply(essays, MARGIN = 1, FUN = paste, collapse=" ")

html <- c( "<a[^>]+>", "class=[\"'][^\"']+[\"']", "&[a-z]+;", "\n", "\\n", "<br ?/>", "</[a-z]+ ?>" )
stop.words <-  c( "a", "am", "an", "and", "as", "at", "are", "be", "but", "can", "do", "for", "have", "i'm", "if", "in", "is", "it", "like", "love", "my", "of", "on", "or", "so", "that", "the", "to", "with", "you", "i" )

html.pat <- paste0( "(", paste(html, collapse = "|"), ")" )
html.pat
stop.words.pat <- paste0( "\\b(", paste(stop.words, collapse = "|"), ")\\b" )
stop.words.pat
essays <- str_replace_all(essays, html.pat, " ")
essays <- str_replace_all(essays, stop.words.pat, " ")

```


# Tokenize essay texts
```{r}

all.tokens <- tokens(essays, what = "word",
                     remove_numbers = TRUE, remove_punct = TRUE,
                     remove_symbols = TRUE, remove_hyphens = TRUE)


# Lower case the tokens.
all.tokens <- tokens_tolower(all.tokens)

# Use quanteda's built-in stopword list for English.
# NOTE - You should always inspect stopword lists for applicability to
#        your problem/domain.

```

#remove stop words
```{r}

all.tokens <- tokens_select(all.tokens, stopwords(),
                            selection = "remove")

```



#Stemming (tokens_wordstem)
```{r}

# Perform stemming on the tokens.
all.tokens <- tokens_wordstem(all.tokens, language = "english")
# remove single-word tokens after stemming. Meaningless
all.tokens <- tokens_select(all.tokens, "^[a-z]$",
                            selection = "remove", valuetype = "regex")



```
# dfm
```{r}
# Create a bag-of-words model (document-term frequency matrix)
all.tokens.dfm <- dfm(all.tokens, tolower = FALSE)
rm(all.tokens)

all.tokens.dfm

sparsity(all.tokens.dfm)

```



(implement your own functions for TF-IDF and then use apply
our function for trimming
```{r}
#TF:
term.frequency<-function(row)
{
 row/sum(row)  
}
#IDF
inverse.doc.freq<-function(col)
{
  corpus.size<-length(col)
  doc.count<-length(which(col>0))
  log10(corpus.size/doc.count)
}
#TF-IDF
tf.idf<-function(x,idf)
{
  x*idf
}
#
all.tokens.df<-apply(all.tokens.dfm, 1,term.frequency )
dim(all.tokens.df)
all.tokens.df[1:20,1:100]
```

DFM matrix
```{r}
dfm.trimmed <- dfm_trim(all.tokens.dfm, min_docfreq = 10, min_termfreq = 20, verbose = TRUE)
dfm.trimmed
# Transform to a matrix and inspect.
# top-50 frequent features
topfeatures(dfm.trimmed, 50)

# Top features of individual documents. WARNING - takes long to run
# top.features <- topfeatures(dfm.trimmed, n = 7, groups = docnames(dfm.trimmed))
#save(top.features, file=file.path( 'dating', "Week5_datingNLP.rdata") )

# features by document frequencies
tail(topfeatures(dfm.trimmed, scheme = "docfreq", n = 200))

#tstat_freq <- textstat_frequency(dfm.trimmed, n = 5, groups = lang)
#head(tstat_freq, 20)

# Transform to a matrix and inspect.
all.tokens.matrix <- as.matrix(dfm.trimmed)
#View(all.tokens.matrix[1:20, 1:100])
dim(all.tokens.matrix)
all.tokens.df<-apply(all.tokens.matrix, 1,term.frequency )
dim(all.tokens.df)
all.tokens.df[1:20,1:100]

```


Train a model to identify male vs. female by their texts:
```{r}
#make data frame

# Setup a the feature data frame with labels.
all.tokens.df <- cbind(Label = profiles$sex, as.data.frame(dfm.trimmed))
#convert(x, to = "data.frame")
# Often, tokenization requires some additional pre-processing
names(all.tokens.df)[c(146, 148, 235, 238)]
# Cleanup column names.
names(all.tokens.df) <- make.names(names(all.tokens.df))
#all.tokens[[357]]


```

Removing the batch effect: find and eliminate “male words” and “female words”
```{r}

```

Cluster the applicants to 2,3,4 and 10 clusters (kmeans)

```{r}

```

